{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = open('names.txt', 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[x.lower() for x in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aamir',\n",
       " 'aaron',\n",
       " 'abbey',\n",
       " 'abbie',\n",
       " 'abbot',\n",
       " 'abbott',\n",
       " 'abby',\n",
       " 'abdel',\n",
       " 'abdul',\n",
       " 'abdulkarim']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 1, \"'\": 2, '-': 3, 'a': 4, 'b': 5, 'c': 6, 'd': 7, 'e': 8, 'f': 9, 'g': 10, 'h': 11, 'i': 12, 'j': 13, 'k': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'q': 20, 'r': 21, 's': 22, 't': 23, 'u': 24, 'v': 25, 'w': 26, 'x': 27, 'y': 28, 'z': 29, '.': 0}\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "#string to int ,int to string implementation\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(stoi)\n",
    "print(len(itos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples:  55869\n"
     ]
    }
   ],
   "source": [
    "# create the dataset\n",
    "xs, ys = [], []\n",
    "for x in names:\n",
    "  chs = ['.'] + list(x) + ['.']\n",
    "  for ch1, ch2 in zip(chs, chs[1:]):\n",
    "    ix1 = stoi[ch1]\n",
    "    ix2 = stoi[ch2]\n",
    "    xs.append(ix1)\n",
    "    ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "print('number of examples: ', len(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function Tensor.type>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#init network\n",
    "W = (torch.randn((len(itos),len(itos)),dtype=torch.float32)*0.01).requires_grad_()\n",
    "W.type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55869])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at iteration 0: 2.4647505283355713\n",
      "Loss at iteration 1: 2.4647490978240967\n",
      "Loss at iteration 2: 2.464747190475464\n",
      "Loss at iteration 3: 2.4647457599639893\n",
      "Loss at iteration 4: 2.4647443294525146\n",
      "Loss at iteration 5: 2.46474289894104\n",
      "Loss at iteration 6: 2.4647409915924072\n",
      "Loss at iteration 7: 2.4647390842437744\n",
      "Loss at iteration 8: 2.464737892150879\n",
      "Loss at iteration 9: 2.464735984802246\n",
      "Loss at iteration 10: 2.4647345542907715\n",
      "Loss at iteration 11: 2.464733123779297\n",
      "Loss at iteration 12: 2.464730978012085\n",
      "Loss at iteration 13: 2.4647295475006104\n",
      "Loss at iteration 14: 2.4647276401519775\n",
      "Loss at iteration 15: 2.464726448059082\n",
      "Loss at iteration 16: 2.4647247791290283\n",
      "Loss at iteration 17: 2.4647233486175537\n",
      "Loss at iteration 18: 2.464721202850342\n",
      "Loss at iteration 19: 2.464719772338867\n",
      "Loss at iteration 20: 2.4647183418273926\n",
      "Loss at iteration 21: 2.4647164344787598\n",
      "Loss at iteration 22: 2.464715003967285\n",
      "Loss at iteration 23: 2.4647135734558105\n",
      "Loss at iteration 24: 2.464711904525757\n",
      "Loss at iteration 25: 2.464709758758545\n",
      "Loss at iteration 26: 2.4647083282470703\n",
      "Loss at iteration 27: 2.4647068977355957\n",
      "Loss at iteration 28: 2.464704990386963\n",
      "Loss at iteration 29: 2.4647035598754883\n",
      "Loss at iteration 30: 2.4647018909454346\n",
      "Loss at iteration 31: 2.46470046043396\n",
      "Loss at iteration 32: 2.464698553085327\n",
      "Loss at iteration 33: 2.4646968841552734\n",
      "Loss at iteration 34: 2.464695692062378\n",
      "Loss at iteration 35: 2.464693784713745\n",
      "Loss at iteration 36: 2.4646923542022705\n",
      "Loss at iteration 37: 2.4646904468536377\n",
      "Loss at iteration 38: 2.464689016342163\n",
      "Loss at iteration 39: 2.4646875858306885\n",
      "Loss at iteration 40: 2.4646856784820557\n",
      "Loss at iteration 41: 2.464684009552002\n",
      "Loss at iteration 42: 2.4646825790405273\n",
      "Loss at iteration 43: 2.4646811485290527\n",
      "Loss at iteration 44: 2.46467924118042\n",
      "Loss at iteration 45: 2.4646778106689453\n",
      "Loss at iteration 46: 2.4646759033203125\n",
      "Loss at iteration 47: 2.464674949645996\n",
      "Loss at iteration 48: 2.4646730422973633\n",
      "Loss at iteration 49: 2.4646711349487305\n",
      "Loss at iteration 50: 2.464669704437256\n",
      "Loss at iteration 51: 2.464668035507202\n",
      "Loss at iteration 52: 2.4646666049957275\n",
      "Loss at iteration 53: 2.464665174484253\n",
      "Loss at iteration 54: 2.464663028717041\n",
      "Loss at iteration 55: 2.4646615982055664\n",
      "Loss at iteration 56: 2.4646599292755127\n",
      "Loss at iteration 57: 2.464658737182617\n",
      "Loss at iteration 58: 2.4646568298339844\n",
      "Loss at iteration 59: 2.4646546840667725\n",
      "Loss at iteration 60: 2.464653253555298\n",
      "Loss at iteration 61: 2.4646518230438232\n",
      "Loss at iteration 62: 2.4646503925323486\n",
      "Loss at iteration 63: 2.464648723602295\n",
      "Loss at iteration 64: 2.464647054672241\n",
      "Loss at iteration 65: 2.4646456241607666\n",
      "Loss at iteration 66: 2.464644193649292\n",
      "Loss at iteration 67: 2.464641809463501\n",
      "Loss at iteration 68: 2.4646403789520264\n",
      "Loss at iteration 69: 2.4646389484405518\n",
      "Loss at iteration 70: 2.464637041091919\n",
      "Loss at iteration 71: 2.4646356105804443\n",
      "Loss at iteration 72: 2.4646341800689697\n",
      "Loss at iteration 73: 2.464632511138916\n",
      "Loss at iteration 74: 2.464630603790283\n",
      "Loss at iteration 75: 2.4646291732788086\n",
      "Loss at iteration 76: 2.464627504348755\n",
      "Loss at iteration 77: 2.464625835418701\n",
      "Loss at iteration 78: 2.4646244049072266\n",
      "Loss at iteration 79: 2.464622735977173\n",
      "Loss at iteration 80: 2.46462082862854\n",
      "Loss at iteration 81: 2.4646193981170654\n",
      "Loss at iteration 82: 2.464617967605591\n",
      "Loss at iteration 83: 2.464616060256958\n",
      "Loss at iteration 84: 2.4646146297454834\n",
      "Loss at iteration 85: 2.464613199234009\n",
      "Loss at iteration 86: 2.464611530303955\n",
      "Loss at iteration 87: 2.4646100997924805\n",
      "Loss at iteration 88: 2.4646081924438477\n",
      "Loss at iteration 89: 2.464606761932373\n",
      "Loss at iteration 90: 2.4646050930023193\n",
      "Loss at iteration 91: 2.4646034240722656\n",
      "Loss at iteration 92: 2.464601993560791\n",
      "Loss at iteration 93: 2.4646003246307373\n",
      "Loss at iteration 94: 2.4645988941192627\n",
      "Loss at iteration 95: 2.46459698677063\n",
      "Loss at iteration 96: 2.4645955562591553\n",
      "Loss at iteration 97: 2.4645936489105225\n",
      "Loss at iteration 98: 2.464592218399048\n",
      "Loss at iteration 99: 2.4645907878875732\n"
     ]
    }
   ],
   "source": [
    "lr=0.1\n",
    "epochs =100\n",
    "# gradient descent\n",
    "for k in range(epochs):\n",
    "  \n",
    "  # forward pass\n",
    "  xenc = F.one_hot(xs, num_classes=len(itos)).float() # input to the network: one-hot encoding\n",
    "  logits = xenc @ W\n",
    "  #implement softmax \n",
    "  counts = logits.exp() \n",
    "  probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "  #loss fn\n",
    "  loss = -probs[torch.arange(len(xs)), ys].log().mean() + 0.01*(W**2).mean()\n",
    "  print(\"Loss at iteration {}: {}\".format(k, loss))\n",
    "  \n",
    "  # backward pass\n",
    "  W.grad = None # set to zero the gradient\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  W.data -= lr * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ssana.\n",
      "vella.\n",
      "indinautiesya.\n",
      "jeno.\n",
      "a.\n",
      "rewieth.\n",
      "beronceosm-y.\n",
      "amara.\n",
      "h.\n",
      "aliria-qlla.\n"
     ]
    }
   ],
   "source": [
    "#sample from model\n",
    "for k in range(10):\n",
    "    res=[]\n",
    "    ix=0\n",
    "    while True:\n",
    "         xenc = F.one_hot(torch.tensor([ix]), num_classes=len(itos)).float()\n",
    "         logits = xenc @ W # predict log-counts\n",
    "         counts = logits.exp() # counts, equivalent to N\n",
    "         p = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "         ix = torch.multinomial(p, num_samples=1, replacement=False).item()\n",
    "         res.append(itos[ix])\n",
    "         if(ix==0):\n",
    "                break\n",
    "    print(''.join(res))\n",
    "         \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
